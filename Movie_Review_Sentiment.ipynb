{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "The objective of this project is to analyse the Movie Review dataset available on ZINDI to develop a NLP machine learning model which when given a review sentence, classifies whether the sentence is of positive, negative, or neutral sentiment. \n",
        "This objective will be accomplished using the CRISP-DM methodology. \n",
        "\n",
        "•\tThis project builds 4 sentiment analysis models using pre-trained models from huggingface.\n",
        "\n",
        "•\tHuggingface is an AI-powered open-source platform for natural language processing (NLP) that provides access to pre-trained models, datasets, and evaluation metrics. It allows developers to easily train, test and deploy natural language processing models and build applications such as chatbots, language translation, and text summarization.\n",
        "\n",
        "The models we will be building are:\n",
        "\n",
        "- [x] Distilbert-Base-uncased: \n",
        "- [x] XLNet-Base-cased: \n",
        "- [x] Roberta-Base-Uncased:\n",
        "- [x] CardiffNLP's Twitter RoBERTa Base:\n"
      ],
      "metadata": {
        "id": "ERfXEwPkCG-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Dependencies"
      ],
      "metadata": {
        "id": "pDIFvuC_FUGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install datasets transformers huggingface_hub\n",
        "!apt-get install git-lfs"
      ],
      "metadata": {
        "id": "ikTnDP7bkP14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7671c98-e790-4b7e-ccd0-cca36f865ad0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.22.4)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (23.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (3.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: tokenizers, xxhash, dill, responses, multiprocess, huggingface_hub, transformers, datasets\n",
            "Successfully installed datasets-2.10.1 dill-0.3.6 huggingface_hub-0.12.1 multiprocess-0.70.14 responses-0.18.0 tokenizers-0.13.2 transformers-4.26.1 xxhash-3.2.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (2.9.2-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 22 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HtsV19rMgmwk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import wordcloud\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, TrainingArguments, Trainer\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import TrainingArguments"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Importing Data\n",
        "The dataset contains movie reviews along with their associated binary sentiment. The core dataset contains 50,000 reviews split evenly into 25k train and 25k test sets. \n",
        "\n",
        "The dataset can be downloaded [from Zindi here](https://zindi.africa/competitions/movie-review-sentiment-classification-challenge/data)\n",
        "\n",
        "The data used in this project is the sole property of Zindi and the competition host. There are restrictions on how to transmit, duplicate, publish, redistribute or otherwise provide or make available any competition data to any party not participating in the Competition (this includes uploading the data to any public site such as Kaggle or GitHub)"
      ],
      "metadata": {
        "id": "u_KkdpwAFkpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(\"Train.csv\")\n",
        "#df_test = pd.read_csv(\"Test.csv\")"
      ],
      "metadata": {
        "id": "XjQogxaOhEIQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "81952169-6024-4d57-a9bc-eeb2b19b98fe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7aa275f5fcf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#df_test = pd.read_csv(\"Test.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "4d--vTrZImQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.info()"
      ],
      "metadata": {
        "id": "FqRWynOKhkO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "id": "CupyW9hTkRKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TmG66EqxkR99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "StHM3tP3kj9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.isnull().sum()\n",
        "#df.isnull()"
      ],
      "metadata": {
        "id": "JMNPwpS8lM68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#positive tweets\n",
        "df_train[df_train['sentiment'] == \"positive\"]['content'].values[:1]"
      ],
      "metadata": {
        "id": "180gjIGLlc7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#positive tweets\n",
        "df_train[df_train['sentiment'] == \"negative\"]['content'].values[:2]"
      ],
      "metadata": {
        "id": "QiPnzK9Sl8Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's count the number of tweets by sentiments\n",
        "sentiment_counts = df_train.groupby(['sentiment']).size()\n",
        "print(sentiment_counts)\n",
        "\n",
        "# Let's visualize the sentiments\n",
        "fig = plt.figure(figsize=(6,6), dpi=100)\n",
        "ax = plt.subplot(111)\n",
        "sentiment_counts.plot.pie(ax=ax, autopct='%1.1f%%', startangle=270, fontsize=12, label=\"\")\n"
      ],
      "metadata": {
        "id": "ADsgn9yxxVBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = df_train[df_train['sentiment'] == \"negative\"].to_string()\n",
        "#Using wordcloud to visualize tweets\n",
        "#words = df_train['content'].to_string()\n",
        "\n",
        "stopwords = set(STOPWORDS)\n",
        "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(words)\n",
        "plt.figure( figsize=(15,10))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mVytx6KY4Rup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using wordcloud to visualize tweets\n",
        "words = df_train['content'].to_string()\n",
        "\n",
        "stopwords = set(STOPWORDS)\n",
        "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(words)\n",
        "plt.figure( figsize=(15,10))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6niuUBAQmSQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "kIcD29FnI6ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['label'] = df_train['sentiment'].map({'positive': 1, 'negative': 0})"
      ],
      "metadata": {
        "id": "feQun5X3mf2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add column to replace values for sentiment as a numeric feature\n",
        "df_train['label1'] = df_train['sentiment'].replace({'positive':1,'negative':0})\n"
      ],
      "metadata": {
        "id": "sLuyMStHogBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train"
      ],
      "metadata": {
        "id": "AqU-pbysnCOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the train data into train, eval\n",
        "train, eval = train_test_split(df_train, test_size=0.2, random_state=42, stratify=df_train['label'])"
      ],
      "metadata": {
        "id": "b_CY_kp1v_u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7Ux3O_a_L5xQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save splitted subsets\n",
        "train.to_csv(\"train_label.csv\", index=False)\n",
        "eval.to_csv(\"eval_label.csv\", index=False)"
      ],
      "metadata": {
        "id": "u--HAKZ5wiyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_dataset('csv', data_files={'train': 'train_label.csv','eval': 'eval_label.csv'}, encoding = \"ISO-8859-1\")"
      ],
      "metadata": {
        "id": "aFtysgevwqH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Creation\n",
        "We will be using a pre-existing deep learning model that has been trained on a large dataset and adapting it to our dataset. This is known as Fine-tuning. It can be an efficient and effective approach for creating high-performing models for a wide range of natural language processing tasks.\n",
        "\n",
        "The advantages of fine-tuning a pretrained model include the ability to quickly and efficiently create high-performing models with limited data, as well as the ability to leverage the knowledge and features learned by the pre-existing model, which can lead to better performance on the new task. We will be finituning the following pre-trained models:\n",
        "\n",
        "- [x] Distilbert-Base-uncased\n",
        "- [x] XLNet-Base-cased\n",
        "- [x] Roberta-Base-uncased\n",
        "- [x] Cardiffnlp Roberta-Base-uncased"
      ],
      "metadata": {
        "id": "FRfRX2W5JdYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL 1.   Fine-tuning a DistilBERT Model \n",
        "\n",
        "---\n",
        "---\n",
        "DistilBERT is a transformer-based language model trained on the same large-scale corpus as BERT. The model has achieved state-of-the-art results on various benchmark datasets, while requiring significantly less computational resources and time."
      ],
      "metadata": {
        "id": "ruAwBB2NKNGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading model tokinizer using AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n"
      ],
      "metadata": {
        "id": "QXz8XLoMwML3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_labels(label):\n",
        "\n",
        "    label = label['label']\n",
        "    num = 0\n",
        "    if label == 0: #'Negative'\n",
        "        num = 0\n",
        "   \n",
        "    elif label == 1: #'Positive'\n",
        "        num = 1\n",
        "\n",
        "    return {'labels': num}\n",
        "\n",
        "def tokenize_data(example):\n",
        "    return tokenizer(example['content'], padding='max_length',truncation=True)\n",
        "\n",
        "# Change the tweets to tokens that the models can exploit\n",
        "dataset = data.map(tokenize_data, batched=True)\n",
        "\n",
        "# Transform\tlabels and remove the useless columns\n",
        "remove_columns = ['label', 'content', 'sentiment']\n",
        "dataset = dataset.map(transform_labels, remove_columns=remove_columns)"
      ],
      "metadata": {
        "id": "RWCViEOAxADT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "LRPwhcWLxMIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lj7qMtRPPifi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the training arguments\n",
        "# Configuring the trianing parameters \n",
        "#access_token = \"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\"\n",
        "repo_name = \"Movie_Review_Sentiment_Analysis\"\n",
        "training_args = TrainingArguments(\n",
        "    \"Movie_Review_Sentiment_Analysis\", \n",
        "    num_train_epochs=5, \n",
        "    load_best_model_at_end=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    #output_dir=repo_name,\n",
        "    push_to_hub=True,)"
      ],
      "metadata": {
        "id": "tL3LuGimxOMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# Loading a pretrain model while specifying the number of labels in our dataset for fine-tuning\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2,id2label={0: 'negative review', 1: 'positive review'})"
      ],
      "metadata": {
        "id": "WnGGiZSyxY8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the evaluation metrics\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "Gvz_k1S6xafT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset['train'].shuffle(seed=10) \n",
        "eval_dataset = dataset['eval'].shuffle(seed=10)"
      ],
      "metadata": {
        "id": "6H7d72vDxtbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#logging into hugging face account \n",
        "from huggingface_hub import notebook_login,  login\n",
        "#notebook_login()\n",
        "login(token=\"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\", add_to_git_credential= True)\n"
      ],
      "metadata": {
        "id": "EZ43EnMibm22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset,compute_metrics=compute_metrics)\n"
      ],
      "metadata": {
        "id": "EQWvVRl9x2-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#converting training data to PyTorch tensors to speed up training and adding padding:\n",
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "azbUuQ66yBm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "MK5BfVuUyUGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ASD=ASDFA"
      ],
      "metadata": {
        "id": "kcV53ZtsSgbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "42o4YiYcyVyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "THuWbHRFyHc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch the final evaluation \n",
        "trainer.evaluate()\n",
        "#tokenizer.save_pretrained"
      ],
      "metadata": {
        "id": "7whklly3eOAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eU53PD8bykNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving model to hub\n",
        "trainer.push_to_hub()"
      ],
      "metadata": {
        "id": "-oElYuJ0yleX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch the final evaluation \n",
        "trainer.evaluate()\n"
      ],
      "metadata": {
        "id": "aeGK6YQayHBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL 2.   Fine-tuning an XLNet Model \n",
        "\n",
        "---\n",
        "---\n",
        "XLNet is a transformer-based language model that uses a permutation language modeling approach. The model has been pre-trained on a large corpus of text and can be fine-tuned on various downstream NLP tasks such as sentiment analysis, text classification\n"
      ],
      "metadata": {
        "id": "E7e1vSuuy4NV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n",
        "\n",
        "def transform_labels(label):\n",
        "\n",
        "    label = label['label']\n",
        "    num = 0\n",
        "    if label == 0: #'Negative'\n",
        "        num = 0\n",
        "   \n",
        "    elif label == 1: #'Positive'\n",
        "        num = 1\n",
        "\n",
        "    return {'labels': num}\n",
        "\n",
        "def tokenize_data(example):\n",
        "    return tokenizer(example['content'], padding='max_length',truncation=True)\n",
        "\n",
        "# Change the tweets to tokens that the models can exploit\n",
        "dataset = data.map(tokenize_data, batched=True)\n",
        "\n",
        "# Transform\tlabels and remove the useless columns\n",
        "remove_columns = ['label', 'content', 'sentiment']\n",
        "dataset = dataset.map(transform_labels, remove_columns=remove_columns)\n",
        "\n",
        "\n",
        "repo_name = \"XLNet_on_Movie_Review_Data\"\n",
        "\n",
        "#defining the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    \"XLNet_on_Movie_Review_Data\", \n",
        "    num_train_epochs=4, \n",
        "    load_best_model_at_end=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "# Loading a pretrain model while specifying the number of labels in our dataset for fine-tuning\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2,id2label={0: 'negative review', 1: 'positive review'})\n",
        "\n",
        "#defining the evaluation metrics\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Launch the training \n",
        "trainer.train()\n",
        "\n"
      ],
      "metadata": {
        "id": "dlkHg915Gh8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL 3.   Fine-tuning a Roberta Model \n",
        "\n",
        "---\n",
        "---\n",
        "RoBERTa is a transformer-based language model introduced by Facebook AI in 2019. It is a variant of the Bert model that is pre-trained on a massive amount of text data, including BooksCorpus (800 million words) and the English Wikipedia (2.5 billion words), using a modified version of the BERT pre-training procedure. \n"
      ],
      "metadata": {
        "id": "Wub-pDKCxViS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", num_labels=2)\n",
        "\n",
        "def transform_labels(label):\n",
        "\n",
        "    label = label['label']\n",
        "    num = 0\n",
        "    if label == 0: #'Negative'\n",
        "        num = 0\n",
        "   \n",
        "    elif label == 1: #'Positive'\n",
        "        num = 1\n",
        "\n",
        "    return {'labels': num}\n",
        "\n",
        "def tokenize_data(example):\n",
        "    return tokenizer(example['content'], padding='max_length',truncation=True)\n",
        "\n",
        "# Change the tweets to tokens that the models can exploit\n",
        "dataset = data.map(tokenize_data, batched=True)\n",
        "\n",
        "# Transform\tlabels and remove the useless columns\n",
        "remove_columns = ['label', 'content', 'sentiment']\n",
        "dataset = dataset.map(transform_labels, remove_columns=remove_columns)\n",
        "\n",
        "\n",
        "repo_name = \"roberta_on_Movie_Review_Data\"\n",
        "\n",
        "#defining the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    \"roberta_on_Movie_Review_Data\", \n",
        "    num_train_epochs=4, \n",
        "    load_best_model_at_end=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "# Loading a pretrain model while specifying the number of labels in our dataset for fine-tuning\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
        "\n",
        "#defining the evaluation metrics\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Launch the training \n",
        "trainer.train()\n",
        "\n"
      ],
      "metadata": {
        "id": "JVkThPwqRJSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL 4.   Fine-tuning a Cardiffnlp Model \n",
        "\n",
        "---\n",
        "---\n",
        "CardiffNLP's Twitter RoBERTa Base model is a pre-trained language model for sentiment analysis on Twitter data. It is based on the RoBERTa architecture.The model has also been fine-tuned on several sentiment analysis tasks to improve its performance on Twitter sentiment analysis.\n"
      ],
      "metadata": {
        "id": "Snp9evaN3QVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\", num_labels=2)\n",
        "\n",
        "def transform_labels(label):\n",
        "\n",
        "    label = label['label']\n",
        "    num = 0\n",
        "    if label == 0: #'Negative'\n",
        "        num = 0\n",
        "   \n",
        "    elif label == 1: #'Positive'\n",
        "        num = 1\n",
        "\n",
        "    return {'labels': num}\n",
        "\n",
        "def tokenize_data(example):\n",
        "    return tokenizer(example['content'], padding='max_length',truncation=True)\n",
        "\n",
        "# Change the tweets to tokens that the models can exploit\n",
        "dataset = data.map(tokenize_data, batched=True)\n",
        "\n",
        "# Transform\tlabels and remove the useless columns\n",
        "remove_columns = ['label', 'content', 'sentiment']\n",
        "dataset = dataset.map(transform_labels, remove_columns=remove_columns)\n",
        "\n",
        "\n",
        "repo_name = \"robertaBase_on_Movie_Review_Data\"\n",
        "\n",
        "#defining the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    \"robertaBase_on_Movie_Review_Data\", \n",
        "    num_train_epochs=4, \n",
        "    load_best_model_at_end=True,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "# Loading a pretrain model while specifying the number of labels in our dataset for fine-tuning\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\", num_labels=2,ignore_mismatched_sizes=True)\n",
        "\n",
        "#defining the evaluation metrics\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Launch the training \n",
        "trainer.train()\n",
        "\n"
      ],
      "metadata": {
        "id": "vr3El2sk6Qr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Gradio APPs"
      ],
      "metadata": {
        "id": "LP0VL9IE3w3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio "
      ],
      "metadata": {
        "id": "-jyBQofB1pbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --Using the inference API\n",
        "The Hugging Face Inference API is a web service provided by Hugging Face that allows developers to access models already saved on the hub via a simple API. The API can be accessed via a RESTful interface, making it easy to integrate with other systems."
      ],
      "metadata": {
        "id": "JJUVqaY84RxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "# Creating a gradio app using the inferene API\n",
        "App = gr.Interface.load(\"huggingface/allevelly/Movie_Review_Sentiment_Analysis\",\n",
        "  title=\"sentiment analysis of tweets on covid19 Vaccines\", description =\"sentiment analysis of tweets on covid19 Vaccines using DistilBERT model\",\n",
        " allow_flagging=False, examples=[[\"Type your messgage about covid vaccines above\"]]\n",
        ")\n",
        "\n",
        "App.launch()\n"
      ],
      "metadata": {
        "id": "5Q0Go-vfBad-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --Using regular inference with pipeline\n",
        "Using regular inference with pipeline in Gradio allows developers to leverage pre-trained models from the Hugging Face Model Hub and use it to create a web application with a user-friendly interface, where users can input data and get predictions.\n",
        "Using the Hugging Face Transformers library, we can load a saved model and its corresponding tokenizer, and then creating a pipeline that can be used to perform regular inference (i.e., making predictions on new data) using that model. This pipeline can then be used to create a Gradio app, which allows users to interact with the model through an intuitive interface.This allows for a more efficient and convenient way to perform inference, as the pipeline can handle tasks such as tokenization, feature extraction, and output formatting. "
      ],
      "metadata": {
        "id": "X_fz7NcF4klZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        " \n",
        "# using pipeline for inference and prediction\n",
        "sentiment_model = pipeline(model=\"allevelly/Movie_Review_Sentiment_Analysis\",tokenizer=tokenizer)\n",
        "sentiment_model(\"Recently shown on cable tv the movie opens with a disclaimer distancing itself from any co-operation of real life persons; that in itself is an eye catcher. Yet the script and acting from the main characters is superb and I found myself engrossed throughout.Due in no small way to the crisp, thoughtful and interesting dialogue.The film is about a meeting on one day between two real life musical 'legends' who formerly composed together then seperated.The film captures the essence of their lives and philosophies, in a story which proffers an explanation for their initial 'split'. What is so impressive is that the actors give such seemingly realistic portrayals of the characters they play,faults and all, that this viewer at least was left believing I was witnessing a true event in almost every detail. The great skill of this play is that with astute writing and fine acting a movie basically about 'two of us' talking can make an excellent picture. Worthy of at least an 8 out of 10\")\n",
        "#sentiment_model(\"Wow, this film was just bloody horrid. SO bad in fact that even though I didn\\'t pay to see it, I still wanted my money back.<br /><br />The film is about nothing intelligible. It\\'s a mish-mash of sci-fi cliche\\'s that were done better by much more skilled film makers. The performances, especially by the leads were over the top in a less endearing Ed Wood sort of way. Speaking of Ed Wood, he\\'d be proud of the character\\'s dialogue. It\\'s just too taciturn with no hint of irony or sense of humor. On top of that, it doesn\\'t make sense, nor does the plot, or lackthereof.<br /><br />The visual effects are okay, but not enough to go \"oh wow, that\\'s cool\" and they just seem to be thrown in to \"be cool\" rather than be a good plot device.<br /><br />The soundtrack was another mishmash of stuff that really never set any sort of mood. Again, it seemed as if the director was just throwing in songs in the film in an effort to \"be cool\".<br /><br />Which brings me to my final point. Perhaps if the director actually worried more about plot, story and dialogue instead of trying to \"be cool\", he wouldn\\'t have made such a dorky cliche\\' of a short film.\")\n"
      ],
      "metadata": {
        "id": "3skHum2sy0b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --Comparing the outputs of the Models using Gradio Parallel\n",
        "Gradio offers the ability to run \"multiple models in parallel\" or run multiple models at the same time, side by side, and to compare their predictions on the same input data. The ability to run multiple models in parallel in Gradio allows developers to easily compare the performance of different models on a given task and make informed decisions about which model to use in their application."
      ],
      "metadata": {
        "id": "tT40NY_I40k7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from gradio.mix import Parallel, Series\n",
        "app1 = gr.Interface.load(\"huggingface/allevelly/Movie_Review_Sentiment_Analysis\")\n",
        "app2 =gr.Interface.load(\"huggingface/allevelly/XLNet_on_Movie_Review_Data\")\n",
        "#app3= gr.Interface(my_language_model,\"text\",\"text\")\n",
        "Parallel(app1,app2).launch()"
      ],
      "metadata": {
        "id": "7124X5061JC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "# Creating a gradio app using the inferene API\n",
        "App = gr.Interface.load(\"huggingface/allevelly/XLNet_on_Movie_Review_Data\",\n",
        "  title=\"sentiment analysis of tweets on covid19 Vaccines\", description =\"sentiment analysis of tweets on covid19 Vaccines using DistilBERT model\",\n",
        " allow_flagging=False, examples=[[\"Type your messgage about covid vaccines above\"]]\n",
        ")\n",
        "\n",
        "App.launch()\n"
      ],
      "metadata": {
        "id": "yjy2rY29Cu6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Biv9JYBw1lA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Streamlit APP"
      ],
      "metadata": {
        "id": "ADMNUbNb5fbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit transformers torch\n",
        "\n",
        "import streamlit as st\n",
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def load_model():\n",
        "    model = pipeline('text-classification', model='allevelly/Movie_Review_Sentiment_Analysis')\n",
        "    return model\n",
        "\n",
        "\n",
        "def main():\n",
        "    st.title(\"Movie Review Sentiment Analysis using Hugging Face API\")\n",
        "    st.write(\"Enter a movie review to classify its sentiment:\")\n",
        "\n",
        "    # Load the model\n",
        "    model = load_model()\n",
        "\n",
        "    # Get user input\n",
        "    text_input = st.text_input(\"Input text\", value='I loved this movie!')\n",
        "\n",
        "    # Classify sentiment\n",
        "    result = model(text_input)\n",
        "\n",
        "    # Display sentiment\n",
        "    if result[0]['label'] == 'NEGATIVE':\n",
        "        st.error(result[0]['score'])\n",
        "    else:\n",
        "        st.success(result[0]['score'])\n",
        "\n",
        "#streamlit run app.py\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8xPfQTuWV6l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mXJ5dhn86RP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Developing an API Endpont using FastAPI\n"
      ],
      "metadata": {
        "id": "S-5ZhqyZVMLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "\n",
        "model = pipeline('text-classification', model='allevelly/Movie_Review_Sentiment_Analysis')\n",
        "\n",
        "\n",
        "@app.post('/predict')\n",
        "async def predict_sentiment(text: str):\n",
        "    result = model(text)\n",
        "    sentiment = result[0]['label']\n",
        "    score = result[0]['score']\n",
        "    return {'sentiment': sentiment, 'score': score}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rTIlrCunYmr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip freeze"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMJUWkmVbjXl",
        "outputId": "48336b54-557a-4c13-ec16-f6fcb9623ddb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "absl-py==1.4.0\n",
            "aeppl==0.0.33\n",
            "aesara==2.7.9\n",
            "aiohttp==3.8.4\n",
            "aiosignal==1.3.1\n",
            "alabaster==0.7.13\n",
            "albumentations==1.2.1\n",
            "altair==4.2.2\n",
            "appdirs==1.4.4\n",
            "argon2-cffi==21.3.0\n",
            "argon2-cffi-bindings==21.2.0\n",
            "arviz==0.12.1\n",
            "astor==0.8.1\n",
            "astropy==4.3.1\n",
            "astunparse==1.6.3\n",
            "async-timeout==4.0.2\n",
            "atomicwrites==1.4.1\n",
            "attrs==22.2.0\n",
            "audioread==3.0.0\n",
            "autograd==1.5\n",
            "Babel==2.12.1\n",
            "backcall==0.2.0\n",
            "backports.zoneinfo==0.2.1\n",
            "beautifulsoup4==4.6.3\n",
            "bleach==6.0.0\n",
            "blis==0.7.9\n",
            "bokeh==2.4.3\n",
            "branca==0.6.0\n",
            "bs4==0.0.1\n",
            "CacheControl==0.12.11\n",
            "cachetools==5.3.0\n",
            "catalogue==2.0.8\n",
            "certifi==2022.12.7\n",
            "cffi==1.15.1\n",
            "cftime==1.6.2\n",
            "chardet==4.0.0\n",
            "charset-normalizer==3.0.1\n",
            "click==8.1.3\n",
            "clikit==0.6.2\n",
            "cloudpickle==2.2.1\n",
            "cmake==3.22.6\n",
            "cmdstanpy==1.1.0\n",
            "colorcet==3.0.1\n",
            "colorlover==0.3.0\n",
            "community==1.0.0b1\n",
            "confection==0.0.4\n",
            "cons==0.4.5\n",
            "contextlib2==0.5.5\n",
            "convertdate==2.4.0\n",
            "crashtest==0.3.1\n",
            "crcmod==1.7\n",
            "cufflinks==0.17.3\n",
            "cupy-cuda11x==11.0.0\n",
            "cvxopt==1.3.0\n",
            "cvxpy==1.2.3\n",
            "cycler==0.11.0\n",
            "cymem==2.0.7\n",
            "Cython==0.29.33\n",
            "dask==2022.2.1\n",
            "datascience==0.17.6\n",
            "datasets==2.10.1\n",
            "db-dtypes==1.0.5\n",
            "dbus-python==1.2.16\n",
            "debugpy==1.6.4\n",
            "decorator==4.4.2\n",
            "defusedxml==0.7.1\n",
            "dill==0.3.6\n",
            "distributed==2022.2.1\n",
            "dlib==19.24.0\n",
            "dm-tree==0.1.8\n",
            "dnspython==2.3.0\n",
            "docutils==0.16\n",
            "dopamine-rl==1.0.5\n",
            "earthengine-api==0.1.342\n",
            "easydict==1.10\n",
            "ecos==2.0.12\n",
            "editdistance==0.5.3\n",
            "en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl\n",
            "entrypoints==0.4\n",
            "ephem==4.1.4\n",
            "et-xmlfile==1.1.0\n",
            "etils==1.0.0\n",
            "etuples==0.3.8\n",
            "fa2==0.3.5\n",
            "fastai==2.7.11\n",
            "fastcore==1.5.28\n",
            "fastdownload==0.0.7\n",
            "fastdtw==0.3.4\n",
            "fastjsonschema==2.16.3\n",
            "fastprogress==1.0.3\n",
            "fastrlock==0.8.1\n",
            "feather-format==0.4.1\n",
            "filelock==3.9.0\n",
            "firebase-admin==5.3.0\n",
            "fix-yahoo-finance==0.0.22\n",
            "Flask==2.2.3\n",
            "flatbuffers==23.1.21\n",
            "folium==0.12.1.post1\n",
            "fonttools==4.38.0\n",
            "frozenlist==1.3.3\n",
            "fsspec==2023.1.0\n",
            "future==0.16.0\n",
            "gast==0.4.0\n",
            "GDAL==3.3.2\n",
            "gdown==4.4.0\n",
            "gensim==3.6.0\n",
            "geographiclib==1.52\n",
            "geopy==1.17.0\n",
            "gin-config==0.5.0\n",
            "glob2==0.7\n",
            "google==2.0.3\n",
            "google-api-core==2.11.0\n",
            "google-api-python-client==2.70.0\n",
            "google-auth==2.16.1\n",
            "google-auth-httplib2==0.1.0\n",
            "google-auth-oauthlib==0.4.6\n",
            "google-cloud-bigquery==3.4.2\n",
            "google-cloud-bigquery-storage==2.18.1\n",
            "google-cloud-core==2.3.2\n",
            "google-cloud-datastore==2.11.1\n",
            "google-cloud-firestore==2.7.3\n",
            "google-cloud-language==2.6.1\n",
            "google-cloud-storage==2.7.0\n",
            "google-cloud-translate==3.8.4\n",
            "google-colab @ file:///colabtools/dist/google-colab-1.0.0.tar.gz\n",
            "google-crc32c==1.5.0\n",
            "google-pasta==0.2.0\n",
            "google-resumable-media==2.4.1\n",
            "googleapis-common-protos==1.58.0\n",
            "googledrivedownloader==0.4\n",
            "graphviz==0.10.1\n",
            "greenlet==2.0.2\n",
            "grpcio==1.51.3\n",
            "grpcio-status==1.48.2\n",
            "gspread==3.4.2\n",
            "gspread-dataframe==3.0.8\n",
            "gym==0.25.2\n",
            "gym-notices==0.0.8\n",
            "h5py==3.1.0\n",
            "HeapDict==1.0.1\n",
            "hijri-converter==2.2.4\n",
            "holidays==0.20\n",
            "holoviews==1.14.9\n",
            "html5lib==1.0.1\n",
            "httpimport==0.5.18\n",
            "httplib2==0.17.4\n",
            "httpstan==4.6.1\n",
            "huggingface-hub==0.12.1\n",
            "humanize==0.5.1\n",
            "hyperopt==0.1.2\n",
            "idna==2.10\n",
            "imageio==2.9.0\n",
            "imagesize==1.4.1\n",
            "imbalanced-learn==0.8.1\n",
            "imblearn==0.0\n",
            "imgaug==0.4.0\n",
            "importlib-metadata==6.0.0\n",
            "importlib-resources==5.12.0\n",
            "imutils==0.5.4\n",
            "inflect==2.1.0\n",
            "intel-openmp==2023.0.0\n",
            "ipykernel==5.3.4\n",
            "ipython==7.9.0\n",
            "ipython-genutils==0.2.0\n",
            "ipython-sql==0.3.9\n",
            "ipywidgets==7.7.1\n",
            "itsdangerous==2.1.2\n",
            "jax==0.4.4\n",
            "jaxlib @ https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.4.4+cuda11.cudnn82-cp38-cp38-manylinux2014_x86_64.whl\n",
            "jieba==0.42.1\n",
            "Jinja2==3.1.2\n",
            "joblib==1.2.0\n",
            "jsonschema==4.3.3\n",
            "jupyter-client==6.1.12\n",
            "jupyter-console==6.1.0\n",
            "jupyter_core==5.2.0\n",
            "jupyterlab-pygments==0.2.2\n",
            "jupyterlab-widgets==3.0.5\n",
            "kaggle==1.5.12\n",
            "keras==2.11.0\n",
            "keras-vis==0.4.1\n",
            "kiwisolver==1.4.4\n",
            "korean-lunar-calendar==0.3.1\n",
            "langcodes==3.3.0\n",
            "libclang==15.0.6.1\n",
            "librosa==0.8.1\n",
            "lightgbm==2.2.3\n",
            "llvmlite==0.39.1\n",
            "lmdb==0.99\n",
            "locket==1.0.0\n",
            "logical-unification==0.4.5\n",
            "LunarCalendar==0.0.9\n",
            "lxml==4.9.2\n",
            "Markdown==3.4.1\n",
            "MarkupSafe==2.1.2\n",
            "marshmallow==3.19.0\n",
            "matplotlib==3.5.3\n",
            "matplotlib-venn==0.11.9\n",
            "miniKanren==1.0.3\n",
            "missingno==0.5.2\n",
            "mistune==0.8.4\n",
            "mizani==0.8.1\n",
            "mkl==2019.0\n",
            "mlxtend==0.14.0\n",
            "more-itertools==9.1.0\n",
            "moviepy==0.2.3.5\n",
            "mpmath==1.2.1\n",
            "msgpack==1.0.4\n",
            "multidict==6.0.4\n",
            "multipledispatch==0.6.0\n",
            "multiprocess==0.70.14\n",
            "multitasking==0.0.11\n",
            "murmurhash==1.0.9\n",
            "music21==5.5.0\n",
            "natsort==5.5.0\n",
            "nbclient==0.7.2\n",
            "nbconvert==6.5.4\n",
            "nbformat==5.7.3\n",
            "netCDF4==1.6.2\n",
            "networkx==3.0\n",
            "nibabel==3.0.2\n",
            "nltk==3.7\n",
            "notebook==6.3.0\n",
            "numba==0.56.4\n",
            "numexpr==2.8.4\n",
            "numpy==1.22.4\n",
            "oauth2client==4.1.3\n",
            "oauthlib==3.2.2\n",
            "opencv-contrib-python==4.6.0.66\n",
            "opencv-python==4.6.0.66\n",
            "opencv-python-headless==4.7.0.72\n",
            "openpyxl==3.0.10\n",
            "opt-einsum==3.3.0\n",
            "osqp==0.6.2.post0\n",
            "packaging==23.0\n",
            "palettable==3.3.0\n",
            "pandas==1.3.5\n",
            "pandas-datareader==0.9.0\n",
            "pandas-gbq==0.17.9\n",
            "pandas-profiling==1.4.1\n",
            "pandocfilters==1.5.0\n",
            "panel==0.14.3\n",
            "param==1.12.3\n",
            "parso==0.8.3\n",
            "partd==1.3.0\n",
            "pastel==0.2.1\n",
            "pathlib==1.0.1\n",
            "pathy==0.10.1\n",
            "patsy==0.5.3\n",
            "pep517==0.13.0\n",
            "pexpect==4.8.0\n",
            "pickleshare==0.7.5\n",
            "Pillow==8.4.0\n",
            "pip-tools==6.6.2\n",
            "platformdirs==3.0.0\n",
            "plotly==5.5.0\n",
            "plotnine==0.10.1\n",
            "pluggy==0.7.1\n",
            "pooch==1.7.0\n",
            "portpicker==1.3.9\n",
            "prefetch-generator==1.0.3\n",
            "preshed==3.0.8\n",
            "prettytable==3.6.0\n",
            "progressbar2==3.38.0\n",
            "prometheus-client==0.16.0\n",
            "promise==2.3\n",
            "prompt-toolkit==2.0.10\n",
            "prophet==1.1.2\n",
            "proto-plus==1.22.2\n",
            "protobuf==3.19.6\n",
            "psutil==5.4.8\n",
            "psycopg2==2.9.5\n",
            "ptyprocess==0.7.0\n",
            "py==1.11.0\n",
            "pyarrow==9.0.0\n",
            "pyasn1==0.4.8\n",
            "pyasn1-modules==0.2.8\n",
            "pycocotools==2.0.6\n",
            "pycparser==2.21\n",
            "pyct==0.5.0\n",
            "pydantic==1.10.5\n",
            "pydata-google-auth==1.7.0\n",
            "pydot==1.3.0\n",
            "pydot-ng==2.0.0\n",
            "pydotplus==2.0.2\n",
            "PyDrive==1.3.1\n",
            "pyerfa==2.0.0.1\n",
            "Pygments==2.6.1\n",
            "PyGObject==3.36.0\n",
            "pylev==1.4.0\n",
            "pymc==4.1.4\n",
            "PyMeeus==0.5.12\n",
            "pymongo==4.3.3\n",
            "pymystem3==0.2.0\n",
            "PyOpenGL==3.1.6\n",
            "pyparsing==3.0.9\n",
            "pyrsistent==0.19.3\n",
            "pysimdjson==3.2.0\n",
            "PySocks==1.7.1\n",
            "pystan==3.3.0\n",
            "pytest==3.6.4\n",
            "python-apt==2.0.1\n",
            "python-dateutil==2.8.2\n",
            "python-louvain==0.16\n",
            "python-slugify==8.0.1\n",
            "python-utils==3.5.2\n",
            "pytz==2022.7.1\n",
            "pyviz-comms==2.2.1\n",
            "PyWavelets==1.4.1\n",
            "PyYAML==6.0\n",
            "pyzmq==23.2.1\n",
            "qdldl==0.1.5.post3\n",
            "qudida==0.0.4\n",
            "regex==2022.6.2\n",
            "requests==2.25.1\n",
            "requests-oauthlib==1.3.1\n",
            "requests-unixsocket==0.2.0\n",
            "resampy==0.4.2\n",
            "responses==0.18.0\n",
            "rpy2==3.5.5\n",
            "rsa==4.9\n",
            "scikit-image==0.19.3\n",
            "scikit-learn==1.2.1\n",
            "scipy==1.10.1\n",
            "screen-resolution-extra==0.0.0\n",
            "scs==3.2.2\n",
            "seaborn==0.11.2\n",
            "Send2Trash==1.8.0\n",
            "shapely==2.0.1\n",
            "six==1.15.0\n",
            "sklearn-pandas==2.2.0\n",
            "smart-open==6.3.0\n",
            "snowballstemmer==2.2.0\n",
            "sortedcontainers==2.4.0\n",
            "soundfile==0.12.1\n",
            "spacy==3.4.4\n",
            "spacy-legacy==3.0.12\n",
            "spacy-loggers==1.0.4\n",
            "Sphinx==3.5.4\n",
            "sphinxcontrib-applehelp==1.0.4\n",
            "sphinxcontrib-devhelp==1.0.2\n",
            "sphinxcontrib-htmlhelp==2.0.1\n",
            "sphinxcontrib-jsmath==1.0.1\n",
            "sphinxcontrib-qthelp==1.0.3\n",
            "sphinxcontrib-serializinghtml==1.1.5\n",
            "SQLAlchemy==1.4.46\n",
            "sqlparse==0.4.3\n",
            "srsly==2.4.6\n",
            "statsmodels==0.13.5\n",
            "sympy==1.7.1\n",
            "tables==3.7.0\n",
            "tabulate==0.8.10\n",
            "tblib==1.7.0\n",
            "tenacity==8.2.2\n",
            "tensorboard==2.11.2\n",
            "tensorboard-data-server==0.6.1\n",
            "tensorboard-plugin-wit==1.8.1\n",
            "tensorflow==2.11.0\n",
            "tensorflow-datasets==4.8.3\n",
            "tensorflow-estimator==2.11.0\n",
            "tensorflow-gcs-config==2.11.0\n",
            "tensorflow-hub==0.12.0\n",
            "tensorflow-io-gcs-filesystem==0.31.0\n",
            "tensorflow-metadata==1.12.0\n",
            "tensorflow-probability==0.19.0\n",
            "termcolor==2.2.0\n",
            "terminado==0.13.3\n",
            "text-unidecode==1.3\n",
            "textblob==0.15.3\n",
            "thinc==8.1.7\n",
            "threadpoolctl==3.1.0\n",
            "tifffile==2023.2.27\n",
            "tinycss2==1.2.1\n",
            "tokenizers==0.13.2\n",
            "toml==0.10.2\n",
            "tomli==2.0.1\n",
            "toolz==0.12.0\n",
            "torch @ https://download.pytorch.org/whl/cu116/torch-1.13.1%2Bcu116-cp38-cp38-linux_x86_64.whl\n",
            "torchaudio @ https://download.pytorch.org/whl/cu116/torchaudio-0.13.1%2Bcu116-cp38-cp38-linux_x86_64.whl\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.14.1\n",
            "torchvision @ https://download.pytorch.org/whl/cu116/torchvision-0.14.1%2Bcu116-cp38-cp38-linux_x86_64.whl\n",
            "tornado==6.2\n",
            "tqdm==4.64.1\n",
            "traitlets==5.7.1\n",
            "transformers==4.26.1\n",
            "tweepy==3.10.0\n",
            "typeguard==2.7.1\n",
            "typer==0.7.0\n",
            "typing_extensions==4.5.0\n",
            "tzlocal==1.5.1\n",
            "uritemplate==4.1.1\n",
            "urllib3==1.26.14\n",
            "vega-datasets==0.9.0\n",
            "wasabi==0.10.1\n",
            "wcwidth==0.2.6\n",
            "webargs==8.2.0\n",
            "webencodings==0.5.1\n",
            "Werkzeug==2.2.3\n",
            "widgetsnbextension==3.6.2\n",
            "wordcloud==1.8.2.2\n",
            "wrapt==1.15.0\n",
            "xarray==2022.12.0\n",
            "xarray-einstats==0.5.1\n",
            "xgboost==1.7.4\n",
            "xkit==0.0.0\n",
            "xlrd==1.2.0\n",
            "xlwt==1.3.0\n",
            "xxhash==3.2.0\n",
            "yarl==1.8.2\n",
            "yellowbrick==1.5\n",
            "zict==2.2.0\n",
            "zipp==3.15.0\n"
          ]
        }
      ]
    }
  ]
}